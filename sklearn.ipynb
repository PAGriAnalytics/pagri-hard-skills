{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import movie_reviews\n",
    "# text\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,CountVectorizer, TfidfVectorizer\n",
    "# classification\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  \n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.svm import SVC\n",
    "# classterization\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation, SpectralClustering, DBSCAN, MiniBatchKMeans\n",
    "# preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, label_binarize, OneHotEncoder\n",
    "# mertrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "# embedding vectors\n",
    "# Важно\n",
    "# It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data)  \n",
    "# to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high.  \n",
    "# This will suppress some noise and speed up the computation of pairwise distances between samples\n",
    "from sklearn.manifold import TSNE\n",
    "# reduce the dimensionality\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "import string\n",
    "import random\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date,datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем набор данных\n",
    "iris_df = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lable encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "если порядок совпадает с ожидаемым, то есть по алфавиту буквы идут и подходит, что будет от 0 до n  \n",
    "или нам не важно какая буква будет с какой цифорой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "l = ['a', 'c', 'b', 'd', 'e']\n",
    "\n",
    "le.fit_transform(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "если нам нужно правильный порядок сделать, то используем label_binarize  \n",
    "label_binarize делает one hot encoding, и если нам нужно от 0 до n, то используем argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 2, 1, 4], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "l = ['bad', 'excellent', 'medium', 'very good', 'very bad']\n",
    "\n",
    "label_binarize(l, classes=['excellent', 'very good', 'medium', 'bad', 'very bad']).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "l = ['bad', 'excellent', 'medium', 'very good', 'very bad']\n",
    "\n",
    "label_binarize(l, classes=['excellent', 'very good', 'medium', 'bad', 'very bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сокращение размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99244289 0.00755711]\n",
      "[6.30061232 0.54980396]\n",
      "\n",
      "[[-0.83849224 -0.54491354]\n",
      " [ 0.54491354 -0.83849224]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "PCA(n_components=2)\n",
    "# Чтобы проверить соотношение дисперсии, объясненную этим СПС, мы можем рассмотреть pca.explained_variance_ratio_ :\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.singular_values_)\n",
    "print()\n",
    "# линейные комбинации\n",
    "# Обратите внимание, что первые два компонента в каждом векторе на несколько порядков больше других, что показывает,   \n",
    "# что СПС признал, что дисперсия содержится главным образом в первых двух столбцах\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after PCA: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных\n",
    "data = datasets.load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Разделение данных на обучающий и тестовый наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Применение PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Обучение SVM (C-Support Vector Classification) на данных после PCA\n",
    "svm = SVC()\n",
    "svm.fit(X_train_pca, y_train)\n",
    "\n",
    "# Оценка производительности модели\n",
    "accuracy = svm.score(X_test_pca, y_test)\n",
    "print(f'Accuracy after PCA: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01570766 0.05122679 0.04998062 0.04795064 0.04539933]\n",
      "0.21026503465070323\n",
      "[35.24105443  4.5981613   4.54200434  4.44866153  4.32887456]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "X_dense = np.random.rand(100, 100)\n",
    "X_dense[:, 2 * np.arange(50)] = 0\n",
    "X = csr_matrix(X_dense)\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "svd.fit(X)\n",
    "TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "print(svd.explained_variance_ratio_)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример данных\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "lemma = nltk.WordNetLemmatizer()\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# Загрузка данных\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "# Подготовка данных\n",
    "texts = [' '.join(doc) for doc, _ in documents]\n",
    "labels = [1 if category == 'pos' else 0 for _, category in documents]\n",
    "prep_text = []\n",
    "# Токенизация и лематизация\n",
    "for sentence in texts:\n",
    "    sentence = sentence.lower()\n",
    "    tokens = word_tokenize(sentence)\n",
    "    lem_tokens = [lemma.lemmatize(token) for token in tokens]\n",
    "    lem_filt_tokens = [token for token in lem_tokens if token not in stop_words and token not in punctuation]\n",
    "    prep_sentence = ' '.join(lem_filt_tokens)\n",
    "    prep_text.append(prep_sentence)\n",
    "   \n",
    "# Создание BoW модели\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(prep_text)\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборку\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow, labels, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8033333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81       304\n",
      "           1       0.81      0.79      0.80       296\n",
      "\n",
      "    accuracy                           0.80       600\n",
      "   macro avg       0.80      0.80      0.80       600\n",
      "weighted avg       0.80      0.80      0.80       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Обучение классификатора\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Оценка классификатора\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "\n",
    "print(classification_report(y_test, predictions, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.835\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84       304\n",
      "           1       0.84      0.82      0.83       296\n",
      "\n",
      "    accuracy                           0.83       600\n",
      "   macro avg       0.84      0.83      0.83       600\n",
      "weighted avg       0.84      0.83      0.83       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Обучение классификатора\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Оценка классификатора\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "\n",
    "print(classification_report(y_test, predictions, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение классификатора\n",
    "classifier = LinearDiscriminantAnalysis()\n",
    "classifier.fit(X_train.toarray(), y_train)\n",
    "\n",
    "# Оценка классификатора\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "\n",
    "print(classification_report(y_test, predictions, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение классификатора\n",
    "classifier = KNeighborsClassifier()\n",
    "classifier.fit(X_train.toarray(), y_train)\n",
    "\n",
    "# Оценка классификатора\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "\n",
    "print(classification_report(y_test, predictions, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Класстеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По ссылке можно посмотреть базовые преимущества и недостатки методов кластеризации <br>\n",
    "https://scikit-learn.org/stable/modules/clustering.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "       2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Описываем модель\n",
    "model = KMeans(n_clusters=3)\n",
    "# Проводим моделирование\n",
    "model.fit(iris_df.data)\n",
    "# Предсказание на всем наборе данных\n",
    "all_predictions = model.predict(iris_df.data)\n",
    "all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример класстеризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbe58ca63fe33f9eeae9e71d10368d2b4a57f2b1b395836210cc60d362c66949"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
