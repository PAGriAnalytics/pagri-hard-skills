{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import movie_reviews\n",
    "# text\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,CountVectorizer, TfidfVectorizer\n",
    "# classification\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso #  модель линейной регрессии со встроенной L1-регуляризацией весов (ограничение на сумму модулей весов).\n",
    "from sklearn.linear_model import Ridge # модель линейной регрессии со встроенной L2-регуляризацией весов (ограничение на сумму квадратов весов).\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# classterization\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation, SpectralClustering, DBSCAN, MiniBatchKMeans\n",
    "# preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, label_binarize, OneHotEncoder\n",
    "# mertrics\n",
    "from sklearn.metrics import accuracy_score, silhouette_score\n",
    "from sklearn.metrics import classification_report\n",
    "# embedding vectors\n",
    "# Важно\n",
    "# It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data)  \n",
    "# to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high.  \n",
    "# This will suppress some noise and speed up the computation of pairwise distances between samples\n",
    "from sklearn.manifold import TSNE\n",
    "# reduce the dimensionality\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "import string\n",
    "import random\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date,datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем набор данных\n",
    "iris_df = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lable encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "если порядок совпадает с ожидаемым, то есть по алфавиту буквы идут и подходит, что будет от 0 до n  \n",
    "или нам не важно какая буква будет с какой цифорой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "l = ['a', 'c', 'b', 'd', 'e']\n",
    "\n",
    "le.fit_transform(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "если нам нужно правильный порядок сделать, то используем label_binarize  \n",
    "label_binarize делает one hot encoding, и если нам нужно от 0 до n, то используем argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 2, 1, 4], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "l = ['bad', 'excellent', 'medium', 'very good', 'very bad']\n",
    "\n",
    "label_binarize(l, classes=['excellent', 'very good', 'medium', 'bad', 'very bad']).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "l = ['bad', 'excellent', 'medium', 'very good', 'very bad']\n",
    "\n",
    "label_binarize(l, classes=['excellent', 'very good', 'medium', 'bad', 'very bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding моно сделать и в пандас"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x': ['a', 'b', 'c'], 'y': ['d', 'e', 'f']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x  y\n",
      "0  a  d\n",
      "1  b  e\n",
      "2  c  f\n",
      "     x_a    x_b    x_c    y_d    y_e    y_f\n",
      "0   True  False  False   True  False  False\n",
      "1  False   True  False  False   True  False\n",
      "2  False  False   True  False  False   True\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "df = pd.get_dummies(df)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор признаков, которые лучше других влияют на модель "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3252302 ,  0.83462377,  0.49750423]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X = [[ 0.87, -1.34,  0.31 ],\n",
    "     [-2.79, -0.02, -0.85 ],\n",
    "     [-1.34, -0.48, -2.55 ],\n",
    "     [ 1.92,  1.48,  0.65 ]]\n",
    "y = [0, 1, 0, 1]\n",
    "selector = SelectFromModel(estimator=LogisticRegression()).fit(X, y)\n",
    "selector.estimator_.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Порог, по которому селектор разбивает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.threshold_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат в вдие булевых значений после сравнения порого и коэффициентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И теперь забираем только важные признаки для этой модели\n",
    "По сути просто берем фичи, у которых истина после разделения по селекторву"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.34],\n",
       "       [-0.02],\n",
       "       [-0.48],\n",
       "       [ 1.48]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "\n",
    "selector = SelectFromModel(estimator=DecisionTreeClassifier()).fit(X, y)\n",
    "dt = DecisionTreeClassifier().fit(X, y)\n",
    "selector.estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.threshold_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сокращение размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99244289 0.00755711]\n",
      "[6.30061232 0.54980396]\n",
      "\n",
      "[[-0.83849224 -0.54491354]\n",
      " [ 0.54491354 -0.83849224]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "PCA(n_components=2)\n",
    "# Чтобы проверить соотношение дисперсии, объясненную этим СПС, мы можем рассмотреть pca.explained_variance_ratio_ :\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.singular_values_)\n",
    "print()\n",
    "# линейные комбинации\n",
    "# Обратите внимание, что первые два компонента в каждом векторе на несколько порядков больше других, что показывает,   \n",
    "# что СПС признал, что дисперсия содержится главным образом в первых двух столбцах\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after PCA: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных\n",
    "data = datasets.load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Разделение данных на обучающий и тестовый наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Применение PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Обучение SVM (C-Support Vector Classification) на данных после PCA\n",
    "svm = SVC()\n",
    "svm.fit(X_train_pca, y_train)\n",
    "\n",
    "# Оценка производительности модели\n",
    "accuracy = svm.score(X_test_pca, y_test)\n",
    "print(f'Accuracy after PCA: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01570766 0.05122679 0.04998062 0.04795064 0.04539933]\n",
      "0.21026503465070323\n",
      "[35.24105443  4.5981613   4.54200434  4.44866153  4.32887456]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "X_dense = np.random.rand(100, 100)\n",
    "X_dense[:, 2 * np.arange(50)] = 0\n",
    "X = csr_matrix(X_dense)\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "svd.fit(X)\n",
    "TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "print(svd.explained_variance_ratio_)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример данных\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "lemma = nltk.WordNetLemmatizer()\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# Загрузка данных\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "# Подготовка данных\n",
    "texts = [' '.join(doc) for doc, _ in documents]\n",
    "labels = [1 if category == 'pos' else 0 for _, category in documents]\n",
    "prep_text = []\n",
    "# Токенизация и лематизация\n",
    "for sentence in texts:\n",
    "    sentence = sentence.lower()\n",
    "    tokens = word_tokenize(sentence)\n",
    "    lem_tokens = [lemma.lemmatize(token) for token in tokens]\n",
    "    lem_filt_tokens = [token for token in lem_tokens if token not in stop_words and token not in punctuation]\n",
    "    prep_sentence = ' '.join(lem_filt_tokens)\n",
    "    prep_text.append(prep_sentence)\n",
    "   \n",
    "# Создание BoW модели\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(prep_text)\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборку\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow, labels, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8033333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81       304\n",
      "           1       0.81      0.79      0.80       296\n",
      "\n",
      "    accuracy                           0.80       600\n",
      "   macro avg       0.80      0.80      0.80       600\n",
      "weighted avg       0.80      0.80      0.80       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Обучение классификатора\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Оценка классификатора\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "\n",
    "print(classification_report(y_test, predictions, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.835\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84       304\n",
      "           1       0.84      0.82      0.83       296\n",
      "\n",
      "    accuracy                           0.83       600\n",
      "   macro avg       0.84      0.83      0.83       600\n",
      "weighted avg       0.84      0.83      0.83       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Обучение классификатора\n",
    "classifier = LogisticRegression(solver=\"liblinear\", max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Оценка классификатора\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "\n",
    "print(classification_report(y_test, predictions, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение классификатора\n",
    "classifier = LinearDiscriminantAnalysis()\n",
    "classifier.fit(X_train.toarray(), y_train)\n",
    "\n",
    "# Оценка классификатора\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "\n",
    "print(classification_report(y_test, predictions, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение классификатора\n",
    "classifier = KNeighborsClassifier()\n",
    "classifier.fit(X_train.toarray(), y_train)\n",
    "\n",
    "# Оценка классификатора\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "\n",
    "print(classification_report(y_test, predictions, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso регуляризация (регрессия)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# прочитайте данные с атрибутами аккаунтов компаний на Facebook и активностью на них\n",
    "fb = pd.read_csv('/datasets/dataset_facebook_cosmetics.csv', sep = ';')\n",
    "# разделяем данные на признаки (матрица X) и целевую переменную (y)\n",
    "X = fb.drop('Total Interactions', axis = 1)\n",
    "y = fb['Total Interactions']\n",
    "# разделяем модель на обучающую и валидационную выборку\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# стандартизируем данные методом StandartScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_st = scaler.transform(X_train)\n",
    "X_test_st = scaler.transform(X_test)\n",
    "model = Ridge() # создаём модель класса Ridge-регрессия\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators — число деревьев, на основании которых будем строить лес, глубину дерева max_depth, размер подвыборки признаков max_features, минимальное количество объектов в узле min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# зададим алгоритм для новой модели на основе алгоритма случайного леса\n",
    "rf_model = RandomForestRegressor(n_estimators = 100)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный бустинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# зададим алгоритм для новой модели на основе алгоритма градиентного бустинга\n",
    "gb_model = GradientBoostingRegressor(n_estimators = 100)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred = gb_model.predict(X_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическя регрессия и установка порога отсечения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем набор данных Iris\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[iris.target != 2]  # Используем только два класса\n",
    "y = iris.target[iris.target != 2]\n",
    "# Разделяем данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем модель логистической регрессии\n",
    "model = LogisticRegression(solver=\"liblinear\", )\n",
    "# Обучаем модель\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "результат predict_proba представляет собой двумерный массив, где каждому объекту из валидационной выборки соответствуют 2 значения: вероятность (а на самом деле, скорее уверенность) принадлежности к классу 0 («событие не наступит») и вероятность принадлежности к классу 1 («событие наступит»). В сумме для каждого объекта эти два числа дают единицу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказываем вероятности для тестовой выборки\n",
    "probabilities = model.predict_proba(X_test)\n",
    "# Предсказываем классы на основе стандартного порога 0.5\n",
    "predictions = model.predict(X_test)\n",
    "# Выводим вероятности и предсказанные классы\n",
    "print(\"Предсказанные вероятности:\\n\", probabilities)\n",
    "print(\"Предсказанные классы:\\n\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В матрице путаницы каждая строка соответствует реальному классу, а каждый столбец — предсказанному классу.  \n",
    "Следовательно True Positive будут значения на главной диагонали."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка точности модели\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score (y_test, predictions)\n",
    "recall = recall_score (y_test, predictions) \n",
    "f1= f1_score(y_test, predictions) \n",
    "print(\"Доля правильных ответов модели:\", accuracy)\n",
    "print(\"Точность модели:\", precision)\n",
    "print(\"Полнота модели:\", recall)\n",
    "print(\"F1-мера модели:\", f1)\n",
    "# Выводим матрицу путаницы и отчет о классификации\n",
    "print(\"Матрица путаницы:\\n\", confusion_matrix(y_test, predictions))\n",
    "print(\"Отчет о классификации:\\n\", classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel() # \"выпрямляем\" матрицу, чтобы вытащить нужные значения "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если нам нужно установить свой порог (например, у нас классы несбалансированы), то делаем так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установите новый порог\n",
    "threshold = 0.7\n",
    "# Предсказание вероятностей для тестовой выборки\n",
    "probabilities = model.predict_proba(X_test)\n",
    "# Классификация на основе нового порога\n",
    "predictions = (probabilities[:, 1] >= threshold).astype(int)\n",
    "# Выводим предсказанные классы\n",
    "print(\"Предсказанные классы с порогом 0.7:\\n\", predictions)\n",
    "# Оценка точности модели с новым порогом\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score (y_test, predictions)\n",
    "recall = recall_score (y_test, predictions) \n",
    "print(\"Доля правильных ответов модели с порогом 0.7:\", accuracy)\n",
    "print(\"Точность модели с порогом 0.7:\", precision)\n",
    "print(\"Полнота модели с порогом 0.7:\", recall)\n",
    "# Выводим матрицу путаницы и отчет о классификации\n",
    "print(\"Матрица путаницы с порогом 0.7:\\n\", confusion_matrix(y_test, predictions))\n",
    "print(\"Отчет о классификации с порогом 0.7:\\n\", classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для бинарного классификатора важно постриоть Roc-Auc кривую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Загрузка данных\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "# Обучение модели\n",
    "model = LogisticRegression(solver=\"liblinear\", random_state=0)\n",
    "model.fit(X, y)\n",
    "# Получение предсказанных вероятностей\n",
    "y_scores = model.predict_proba(X)[:, 1]\n",
    "# Вычисление ROC-кривой\n",
    "fpr, tpr, thresholds = roc_curve(y, y_scores)\n",
    "# Вычисление AUC\n",
    "roc_auc = roc_auc_score(y, y_scores)\n",
    "# Построение ROC-кривой с использованием Plotly\n",
    "fig = go.Figure()\n",
    "# Добавляем линию ROC-кривой\n",
    "fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name='ROC Curve (AUC = {:.2f})'.format(roc_auc),\n",
    "                          line=dict(color='blue', width=2)))\n",
    "# Добавляем линию случайного угадывания\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Random Guessing',\n",
    "                          line=dict(color='red', dash='dash')))\n",
    "# Настройка графика\n",
    "fig.update_layout(title='Receiver Operating Characteristic (ROC) Curve',\n",
    "                  xaxis_title='False Positive Rate',\n",
    "                  yaxis_title='True Positive Rate',\n",
    "                  xaxis=dict(range=[0, 1]),\n",
    "                  yaxis=dict(range=[0, 1]),\n",
    "                  showlegend=True)\n",
    "# Отображение графика\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Класстеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По ссылке можно посмотреть базовые преимущества и недостатки методов кластеризации <br>\n",
    "https://scikit-learn.org/stable/modules/clustering.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В переменной labels сохраняются индексы предложенных алгоритмом групп. Алгоритм случайно назначает номер определённой группе, поэтому искать в этой цифре какой-то смысл не нужно: группа с индексом 2 не «ближе» к группе с индексом 3, чем группа 1. Важно, что объекты, которым модель присвоила один и тот же индекс, относятся к одному кластеру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1,\n",
       "       1, 2, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris_df.data\n",
    "# display(X)\n",
    "sc = StandardScaler()\n",
    "X_sc = sc.fit_transform(X)\n",
    "# Описываем модель\n",
    "model = KMeans(n_clusters=3)\n",
    "# Проводим моделирование\n",
    "model.fit(X_sc)\n",
    "# Предсказание на всем наборе данных\n",
    "all_predictions = model.predict(X_sc)\n",
    "all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки качества модели кластеризации  часто используют метрику силуэта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значение метрики силуэта принимает значения от -1 до 1. Чем ближе к 1, тем качественнее кластеризация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.4798814508199817)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(X_sc, all_predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Агломеративная иерархическая кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Суть иерархической кластеризации проста. Когда мы задали функцию расстояния, можно вычислить матрицу расстояний между всеми объектами, в ячейках которой будет попарное расстояние между двумя объектами. А ещё учтены все признаки объектов, а не только два. Например:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основе этой матрицы можно последовательно объединять близкие кластеры. По ней оценить близость отдельных стран, быстро найти самые близкие или самые удалённые друг от друга на глаз сложно. Однако расстояния между объектами и сама агломеративная иерархическая кластеризация хорошо визуализируются на специальных графиках — дендрограммах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сложность агломеративной кластеризации заключается не в устройстве самого алгоритма, а в вычислениях, которые машина совершает для построения дендрограммы. Расчёты попарных расстояний могут занять очень много времени. Потому при решении задачи кластеризации полезно строить дендрограмму на случайной подвыборке, а после оценки оптимального числа кластеров запустить более быстрый алгоритм K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция linkage позволяет выбрать метод агломерации, который определяет, как вычисляется расстояние между кластерами. Некоторые из наиболее распространенных методов:\n",
    "- Ward: Минимизирует сумму квадратов внутри кластеров. Это наиболее часто используемый метод.\n",
    "- Single: Минимизирует расстояние между ближайшими объектами в разных кластерах.\n",
    "- Complete: Минимизирует расстояние между самыми удаленными объектами в разных кластерах.\n",
    "- Average: Использует среднее расстояние между всеми парами объектов в разных кластерах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждая строка матрицы на выходе функции linkage содержит три значения\n",
    "- Индексы объединяемых кластеров\n",
    "    - Первые два значения в строке представляют собой индексы объединяемых кластеров. Если оба индекса меньше, чем количество наблюдений в исходных данных, это означает, что они ссылаются на отдельные наблюдения. Если один из индексов больше или равен количеству наблюдений, это означает, что он ссылается на уже объединенный кластер.\n",
    "    - Например, если у вас есть 5 наблюдений, индексы от 0 до 4 будут ссылаться на эти наблюдения, а индексы 5 и выше будут ссылаться на объединенные кластеры.\n",
    "- Расстояние между объединяемыми кластерами:\n",
    "    - Третье значение в строке — это расстояние между двумя объединяемыми кластерами. Это расстояние определяется в соответствии с выбранным методом агломерации (например, метод Уорда, метод ближайшего соседа и т.д.).\n",
    "- Количество объектов в новом объединенном кластере        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дендограмма — это инструмент визуализации, который используется для представления иерархической структуры кластеров, полученных в результате агломеративной иерархической кластеризации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.figure_factory as ff\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Загрузка данных\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "# display(X)\n",
    "sc = StandardScaler()\n",
    "X_sc = sc.fit_transform(X)\n",
    "# Применение агломеративной иерархической кластеризации\n",
    "model = AgglomerativeClustering(n_clusters=3)\n",
    "model.fit(X_sc)\n",
    "\n",
    "# Получение меток кластеров\n",
    "labels = model.labels_\n",
    "# Вычисление иерархической структуры для дендограммы\n",
    "linked = linkage(X_sc, method='ward')\n",
    "# Дендограмма в matplotlib\n",
    "# plt.figure(figsize=(15, 10))  \n",
    "# dendrogram(linked, orientation='top')\n",
    "# plt.show() \n",
    "# Дендограмма в plotly\n",
    "fig = ff.create_dendrogram(linked, orientation='bottom')\n",
    "fig.update_layout(title='Dendrogram for Scaled Random Data',\n",
    "                  xaxis_title='Distance',\n",
    "                  yaxis_title='Observations',\n",
    "                  width=800,\n",
    "                  height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) — это алгоритм кластеризации, который группирует точки на основе их плотности. Он эффективно выявляет кластеры произвольной формы и может обрабатывать шумовые данные, что делает его популярным в различных областях, таких как анализ геопространственных данных, управление дорожным движением и маркетинг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры:\n",
    "\n",
    "- Эпсилон (ε): Максимальное расстояние, на котором точки могут считаться соседями.\n",
    "- MinPoints: Минимальное количество точек, необходимых для формирования плотного региона.\n",
    "\n",
    "Классификация точек:\n",
    "    \n",
    "- Ядровые точки: Точки, которые имеют по крайней мере MinPoints соседей в пределах ε.\n",
    "- Пограничные точки: Точки, которые находятся в пределах ε от ядровой точки, но не имеют достаточного количества соседей.\n",
    "- Шумовые точки: Точки, которые не являются ни ядровыми, ни пограничными.    \n",
    "\n",
    "Как работает DBSCAN:\n",
    "\n",
    "- Алгоритм начинает с произвольной точки и определяет, является ли она ядровой.\n",
    "- Если точка является ядровой, она и все ее соседние точки добавляются в кластер.\n",
    "- Процесс повторяется для всех соседей, пока не будут обработаны все точки в кластере.\n",
    "- Если точка не является ядровой, она помечается как шум."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение параметра ε (эпсилон) в алгоритме DBSCAN является важным шагом, так как он влияет на качество кластеризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод \"k-Nearest Neighbors\" (k-NN):  \n",
    "Этот метод основан на анализе расстояний до k ближайших соседей. Обычно выбирается значение k, равное MinPoints.\n",
    "\n",
    "Шаги:\n",
    "- Для каждой точки в наборе данных вычисляется расстояние до ее k ближайших соседей.\n",
    "- Эти расстояния сортируются, и строится график (или \"кривая\") расстояний.\n",
    "- На графике ищется \"изгиб\" (или \"локоть\"), который указывает на подходящее значение ε. Этот изгиб соответствует переходу от плотных областей к разреженным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод \"Silhouette Score\":  \n",
    "Этот метод оценивает качество кластеризации, используя силуэтный коэффициент, который измеряет, насколько хорошо каждая точка соответствует своему кластеру по сравнению с другими кластерами.\n",
    "\n",
    "Шаги:\n",
    "- Примените DBSCAN с различными значениями ε.\n",
    "- Для каждого значения ε вычислите силуэтный коэффициент.\n",
    "- Выберите значение ε, которое дает наивысший силуэтный коэффициент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MinPoints — это минимальное количество точек, необходимое для того, чтобы область считалась плотной (ядровой точкой). Обычно это значение выбирается на основе понимания данных и может варьироваться в зависимости от конкретной задачи. Например, если вы ожидаете, что кластеры будут содержать как минимум 5 точек, вы можете установить MinPoints = 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример использования метода k-NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Пример данных\n",
    "X = np.array([[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]])\n",
    "\n",
    "# Определение k\n",
    "k = 2  # Обычно выбирается равным MinPoints\n",
    "\n",
    "# Вычисление расстояний до k ближайших соседей\n",
    "nbrs = NearestNeighbors(n_neighbors=k).fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)\n",
    "\n",
    "# Сортировка расстояний\n",
    "distances = np.sort(distances[:, k-1], axis=0)\n",
    "\n",
    "# Построение графика\n",
    "plt.plot(distances)\n",
    "plt.xlabel('Точки')\n",
    "plt.ylabel('Расстояние до k-го ближайшего соседа')\n",
    "plt.title('График расстояний для выбора ε')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# Пример данных\n",
    "X = np.array([[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]])\n",
    "\n",
    "# Применение DBSCAN\n",
    "dbscan = DBSCAN(eps=3, min_samples=2).fit(X)\n",
    "\n",
    "# Получение меток кластеров\n",
    "labels = dbscan.labels_\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# определим функцию, которая будет выводить наши метрики\n",
    "def print_all_metrics(y_true, y_pred, y_proba, title='Метрики классификации'):\n",
    "    print(title)\n",
    "    print('\\tAccuracy: {:.2f}'.format(accuracy_score(y_true, y_pred)))\n",
    "    print('\\tPrecision: {:.2f}'.format(precision_score(y_true, y_pred)))\n",
    "    print('\\tRecall: {:.2f}'.format(recall_score(y_true, y_pred)))\n",
    "    print('\\tF1: {:.2f}'.format(f1_score(y_true, y_pred)))\n",
    "    print('\\tROC_AUC: {:.2f}'.format(roc_auc_score(y_true, y_proba)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pagri-projects-W8_aXYna-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
