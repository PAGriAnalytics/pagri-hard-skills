{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zjffdu.medium.com/learn-spark-on-zeppelin-in-docker-container-9f3f7b2db230  \n",
    "https://zjffdu.medium.com/learn-pyspark-in-apache-zeppelin-notebook-part-1-3214abede98a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.artofba.com/post/integration-dealing-with-inconsistent-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "готовый образ со спарком  \n",
    "https://www.javaadvent.com/2019/12/running-apache-zeppelin-on-the-cloud.html  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/neomatrix369/awesome-ai-ml-dl/tree/master/examples/apache-zeppelin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "$ docker pull neomatrix369/zeppelin:0.1\n",
    "$ ./runZeppelinDockerContainer.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее нужно скопировать файл `runZeppelinDockerContainer.sh` с гитхаба ссылка выше   \n",
    "и поместить куда нужно, теперь входим в папку с этим файлом и зпускаем команду в докере    \n",
    "`IMAGE_VERSION=0.2 ./runZeppelinDockerContainer.sh`  \n",
    "В идеале в этой папке с файлом должны быть директория /notebooks и /logs  \n",
    "Они смонтируются в контейнер  \n",
    "В этом файле это указано   \n",
    "Контейнер будет создан"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно  \n",
    "В zeppelin нужно указывать правильный интерпретатор в ячейке  \n",
    "Например, если мы хотим писать код на питоне и использовать спарк,  \n",
    "то пишем %spark.pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы работал спарк, его нужно либо скопировать в контейнер и разархивировать и указать путь до этой папки в spark path  \n",
    ", либо распокавать архив спарка на хост машине и смонтировать папку в контейнере к ней и эту папку контейнера указать в spark path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "можно распаковать в папку, войти в нее на хотсе в докере и выполнить команду  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker cp name_foldoer_with_spark /opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтобы строить графики matplotlib и seaborn\n",
    "%python.ipython\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly\n",
    "%python.ipython\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=[2, 1, 4, 3]))\n",
    "fig.add_trace(go.Bar(y=[1, 4, 3, 2]))\n",
    "fig.update_layout(title = 'Hello Figure')\n",
    "\n",
    "print(\"%html {0}\".format(fig.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python интерпретатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует удобный интерпретатор %python.sql, который по своему использованию похож на Apache Spark в Zeppelin  \n",
    "и позволяет использовать язык SQL для запроса к датафреймам Pandas и визуализации результатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.arenadata.io/aaw/Zeppelin/Python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pandasql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'col1': [1,2,3], 'col2': [11,22,33]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark интерпретатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/api/python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "df = spark.createDataFrame([(1, 2, 'Sam'), (2, 3, 'Tom'), (4, 5, 'Nick')]).toDF('id', 'number', 'name')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df1 = spark.read.json(\"/srv/data/example/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df1 = spark.read.csv(\"/srv/data/example/people.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "подключение к базе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:gs://239.0.0.1:41999/defaultCluster/public\").option(\"user\", \"admin\").option(\"password\", \"admin\").option(\"driver\", \"com.toshiba.mwcloud.gs.sql.Driver\").option(\"dbtable\", \"csh101_T101\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the PostgreSQL JDBC Driver from https://jdbc.postgresql.org/download/\n",
    "\n",
    "# Then replace the database configuration values by yours.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.jars\", \"/path_to_postgresDriver/postgresql-42.2.5.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/databasename\") \\\n",
    "    .option(\"dbtable\", \"tablename\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbe58ca63fe33f9eeae9e71d10368d2b4a57f2b1b395836210cc60d362c66949"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
